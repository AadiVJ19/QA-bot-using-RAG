{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X8WX7fjR3Hg",
        "outputId": "c3f0d6d2-e822-4072-8b25-72e27199c0c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai pinecone langchain-community tiktoken python-dotenv langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS_LWp2NSCOf",
        "outputId": "250845c1-523d-4356-9ca2-88267a8cb65b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import pinecone\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from dotenv import load_dotenv"
      ],
      "metadata": {
        "id": "pD9ALpUBSHDF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# This will prompt you to enter your API keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = input(\"Enter your OpenAI API key: \").strip()\n",
        "os.environ[\"PINECONE_API_KEY\"] = input(\"Enter your Pinecone API key: \").strip()\n",
        "\n",
        "# Verify the keys are set\n",
        "print(\"\\nAPI Keys have been set successfully!\")\n",
        "print(\"OPENAI_API_KEY:\", \"***\" + (os.getenv(\"OPENAI_API_KEY\")[-4:] if os.getenv(\"OPENAI_API_KEY\") else \"not set\"))\n",
        "print(\"PINECONE_API_KEY:\", \"***\" + (os.getenv(\"PINECONE_API_KEY\")[-4:] if os.getenv(\"PINECONE_API_KEY\") else \"not set\"))"
      ],
      "metadata": {
        "id": "JaAxJqRWTPHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Initialize the Pinecone client\n",
        "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
        "\n",
        "# Create Pinecone index name\n",
        "index_name = \"qa-bot-index\"\n",
        "\n",
        "# Check if index exists, create if not\n",
        "existing_indexes = [index.name for index in pc.list_indexes()]\n",
        "print(\"Existing indexes:\", existing_indexes)\n",
        "\n",
        "if index_name not in existing_indexes:\n",
        "    print(f\"Creating index: {index_name}\")\n",
        "    try:\n",
        "        # Try with gcp-starter region first (commonly available in free tier)\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=1536,  # OpenAI embedding dimension\n",
        "            metric='cosine',\n",
        "            spec=ServerlessSpec(\n",
        "                cloud='gcp',  # Changed to GCP\n",
        "                region='us-central1'  # GCP region that works with free tier\n",
        "            )\n",
        "        )\n",
        "        print(f\"Index {index_name} created successfully in GCP us-central1!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error with GCP: {str(e)}\")\n",
        "        print(\"Trying AWS us-east-1...\")\n",
        "        try:\n",
        "            # If GCP fails, try AWS us-east-1\n",
        "            pc.create_index(\n",
        "                name=index_name,\n",
        "                dimension=1536,\n",
        "                metric='cosine',\n",
        "                spec=ServerlessSpec(\n",
        "                    cloud='aws',\n",
        "                    region='us-east-1'  # AWS region that works with free tier\n",
        "                )\n",
        "            )\n",
        "            print(f\"Index {index_name} created successfully in AWS us-east-1!\")\n",
        "        except Exception as e2:\n",
        "            print(f\"Error with AWS: {str(e2)}\")\n",
        "            print(\"\\nPlease check your Pinecone console for available regions in your plan.\")\n",
        "            raise\n",
        "\n",
        "# Get the index\n",
        "index = pc.Index(index_name)\n",
        "print(f\"Connected to index: {index_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3cC_PMzUOso",
        "outputId": "e3bbbd95-c8bc-4298-af40-03aea8a05ce8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing indexes: []\n",
            "Creating index: qa-bot-index\n",
            "Error with GCP: (400)\n",
            "Reason: Bad Request\n",
            "HTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=utf-8', 'access-control-allow-origin': '*', 'vary': 'origin,access-control-request-method,access-control-request-headers', 'access-control-expose-headers': '*', 'x-pinecone-api-version': '2025-04', 'x-cloud-trace-context': '27875f72ed1e7128ed8e5864b41379bb', 'date': 'Fri, 04 Jul 2025 16:43:54 GMT', 'server': 'Google Frontend', 'Content-Length': '202', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\n",
            "HTTP response body: {\"error\":{\"code\":\"INVALID_ARGUMENT\",\"message\":\"Bad request: Your free plan does not support indexes in the us-central1 region of gcp. To create indexes in this region, upgrade your plan.\"},\"status\":400}\n",
            "\n",
            "Trying AWS us-east-1...\n",
            "Index qa-bot-index created successfully in AWS us-east-1!\n",
            "Connected to index: qa-bot-index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Pinecone as PineconeVectorStore\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "import os\n",
        "\n",
        "def load_documents(file_path):\n",
        "    \"\"\"Load and split documents into chunks.\"\"\"\n",
        "    # Load document\n",
        "    loader = TextLoader(file_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "    return text_splitter.split_documents(documents)\n",
        "\n",
        "def create_vector_store(documents, index):\n",
        "    \"\"\"Create and return a vector store from documents.\"\"\"\n",
        "    # Initialize OpenAI embeddings\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "    # Create and store vectors in Pinecone\n",
        "    vector_store = PineconeVectorStore.from_documents(\n",
        "        documents=documents,\n",
        "        embedding=embeddings,\n",
        "        index_name=index.name\n",
        "    )\n",
        "    return vector_store\n",
        "\n",
        "print(\"Document processing functions defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9j-EfNdUvwU",
        "outputId": "fd20593f-8b8f-45bb-8c26-88ae6e3d78bf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document processing functions defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample document\n",
        "sample_text = \"\"\"# Company FAQ\n",
        "\n",
        "## General Questions\n",
        "Q: What are your business hours?\n",
        "A: Our business hours are from 9:00 AM to 6:00 PM, Monday through Friday.\n",
        "\n",
        "Q: How can I contact support?\n",
        "A: You can reach our support team at support@example.com or call us at (555) 123-4567.\n",
        "\n",
        "## Product Information\n",
        "Q: What products do you offer?\n",
        "A: We offer a range of software solutions including AI tools, data analytics platforms, and cloud services.\n",
        "\n",
        "Q: Do you offer a free trial?\n",
        "A: Yes, we offer a 14-day free trial for all our products.\n",
        "\"\"\"\n",
        "\n",
        "# Save the sample text to a file\n",
        "with open(\"sample_faq.txt\", \"w\") as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "print(\"Sample document created: sample_faq.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKxqv6JSUy58",
        "outputId": "f1bade8e-4616-4de1-e83d-3415cef8b6ea"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample document created: sample_faq.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install the required package\n",
        "!pip install -q langchain-openai\n",
        "\n",
        "# Then update the imports\n",
        "from langchain_openai import OpenAIEmbeddings  # Updated import\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Pinecone as PineconeVectorStore\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "import os\n",
        "\n",
        "def load_documents(file_path):\n",
        "    \"\"\"Load and split documents into chunks.\"\"\"\n",
        "    # Load document\n",
        "    loader = TextLoader(file_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "    return text_splitter.split_documents(documents)\n",
        "\n",
        "def create_vector_store(documents, index_name):\n",
        "    \"\"\"Create and return a vector store from documents.\"\"\"\n",
        "    # Initialize OpenAI embeddings\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "    # Create and store vectors in Pinecone\n",
        "    vector_store = PineconeVectorStore.from_documents(\n",
        "        documents=documents,\n",
        "        embedding=embeddings,\n",
        "        index_name=index_name\n",
        "    )\n",
        "    return vector_store\n",
        "\n",
        "print(\"Document processing functions defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xlnpe_69U20j",
        "outputId": "8acc0312-0d7c-48ac-822d-faa8c0eb5f90"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document processing functions defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's make sure we have all the right packages\n",
        "!pip install -q langchain-openai pinecone-client\n",
        "\n",
        "# Import required libraries\n",
        "import os\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Pinecone as PineconeVectorStore\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from pinecone import Pinecone\n",
        "\n",
        "# Initialize Pinecone client\n",
        "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
        "index_name = \"qa-bot-index\"\n",
        "\n",
        "def load_documents(file_path):\n",
        "    \"\"\"Load and split documents into chunks.\"\"\"\n",
        "    # Load document\n",
        "    loader = TextLoader(file_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "    return text_splitter.split_documents(documents)\n",
        "\n",
        "def create_vector_store(documents, index_name):\n",
        "    \"\"\"Create and return a vector store from documents.\"\"\"\n",
        "    # Initialize OpenAI embeddings\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "    # Create and store vectors in Pinecone\n",
        "    vector_store = PineconeVectorStore.from_documents(\n",
        "        documents=documents,\n",
        "        embedding=embeddings,\n",
        "        index_name=index_name\n",
        "    )\n",
        "    return vector_store\n",
        "\n",
        "print(\"Document processing functions defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IogWUVvJVIAD",
        "outputId": "3d224f7a-6a2b-49e5-9114-b168c9e5679e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document processing functions defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q langchain-huggingface sentence-transformers faiss-cpu torch\n",
        "\n",
        "# Import required libraries\n",
        "import os\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load a local embedding model\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': device}\n",
        ")\n",
        "\n",
        "# Load a local LLM with better configuration\n",
        "model_name = \"gpt2\"  # Using a small model for demonstration\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Configure the text generation pipeline with better parameters\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=100,  # Increased from default\n",
        "    temperature=0.7,     # Added for better response quality\n",
        "    do_sample=True,      # Enable sampling for more diverse responses\n",
        "    device=0 if device == \"cuda\" else -1\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "print(\"Models loaded successfully!\")\n",
        "\n",
        "def load_documents(file_path):\n",
        "    \"\"\"Load and split documents into chunks.\"\"\"\n",
        "    loader = TextLoader(file_path)\n",
        "    documents = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "    return text_splitter.split_documents(documents)\n",
        "\n",
        "def create_vector_store(documents):\n",
        "    \"\"\"Create and return a vector store from documents.\"\"\"\n",
        "    return FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "# Create a sample document\n",
        "sample_text = \"\"\"# Company FAQ\n",
        "\n",
        "## General Questions\n",
        "Q: What are your business hours?\n",
        "A: Our business hours are from 9:00 AM to 6:00 PM, Monday through Friday.\n",
        "\n",
        "Q: How can I contact support?\n",
        "A: You can reach our support team at support@example.com or call us at (555) 123-4567.\n",
        "\n",
        "## Product Information\n",
        "Q: What products do you offer?\n",
        "A: We offer a range of software solutions including AI tools, data analytics platforms, and cloud services.\n",
        "\n",
        "Q: Do you offer a free trial?\n",
        "A: Yes, we offer a 14-day free trial for all our products.\n",
        "\"\"\"\n",
        "\n",
        "# Save the sample text to a file\n",
        "with open(\"sample_faq.txt\", \"w\") as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "print(\"Sample document created: sample_faq.txt\")\n",
        "\n",
        "# Load and process the document\n",
        "try:\n",
        "    documents = load_documents(\"sample_faq.txt\")\n",
        "    print(f\"Loaded {len(documents)} document chunks\")\n",
        "\n",
        "    # Create vector store\n",
        "    vector_store = create_vector_store(documents)\n",
        "    print(\"Vector store created successfully using FAISS!\")\n",
        "\n",
        "    # Create the QA chain\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "    print(\"QA system is ready! You can now ask questions about the document.\")\n",
        "\n",
        "    def ask_question(question):\n",
        "        try:\n",
        "            # Updated to use the new invoke method\n",
        "            result = qa_chain.invoke({\"query\": question})\n",
        "            print(f\"\\nQuestion: {question}\")\n",
        "            print(f\"Answer: {result['result']}\")\n",
        "\n",
        "            if 'source_documents' in result and result['source_documents']:\n",
        "                print(\"\\nSources:\")\n",
        "                for i, doc in enumerate(result['source_documents']):\n",
        "                    print(f\"{i+1}. {doc.page_content[:100]}...\")\n",
        "            else:\n",
        "                print(\"\\nNo sources found for this answer.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError processing question: {str(e)}\")\n",
        "            print(\"Trying with a simpler approach...\")\n",
        "            # Fallback to direct retrieval if the QA chain fails\n",
        "            docs = vector_store.similarity_search(question, k=1)\n",
        "            if docs:\n",
        "                print(f\"\\nMost relevant information found:\")\n",
        "                print(docs[0].page_content)\n",
        "            else:\n",
        "                print(\"No relevant information found in the documents.\")\n",
        "\n",
        "    # Test the QA system\n",
        "    questions = [\n",
        "        \"What are your business hours?\",\n",
        "        \"Do you offer a free trial?\",\n",
        "        \"How can I contact support?\",\n",
        "        \"What products do you offer?\"\n",
        "    ]\n",
        "\n",
        "    for question in questions:\n",
        "        ask_question(question)\n",
        "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    # Interactive mode\n",
        "    print(\"Enter 'quit' to exit the QA system.\")\n",
        "    while True:\n",
        "        user_question = input(\"\\nAsk a question about the document: \")\n",
        "        if user_question.lower() == 'quit':\n",
        "            break\n",
        "        ask_question(user_question)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "    print(\"\\nTroubleshooting steps:\")\n",
        "    print(\"1. Check if you have enough disk space (at least 2GB free)\")\n",
        "    print(\"2. Make sure you have a stable internet connection\")\n",
        "    print(\"3. Try restarting the runtime (Runtime > Restart runtime) and running the code again\")\n",
        "    print(\"4. If the issue persists, try using a smaller model or different parameters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYxU6m2zVhBA",
        "outputId": "d1f00ecf-292a-4d39-96f2-7d9ac52fa7cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models loaded successfully!\n",
            "Sample document created: sample_faq.txt\n",
            "Loaded 1 document chunks\n",
            "Vector store created successfully using FAISS!\n",
            "QA system is ready! You can now ask questions about the document.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: What are your business hours?\n",
            "Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "# Company FAQ\n",
            "\n",
            "## General Questions\n",
            "Q: What are your business hours?\n",
            "A: Our business hours are from 9:00 AM to 6:00 PM, Monday through Friday.\n",
            "\n",
            "Q: How can I contact support?\n",
            "A: You can reach our support team at support@example.com or call us at (555) 123-4567.\n",
            "\n",
            "## Product Information\n",
            "Q: What products do you offer?\n",
            "A: We offer a range of software solutions including AI tools, data analytics platforms, and cloud services.\n",
            "\n",
            "Q: Do you offer a free trial?\n",
            "A: Yes, we offer a 14-day free trial for all our products.\n",
            "\n",
            "Question: What are your business hours?\n",
            "Helpful Answer:\n",
            "\n",
            "Q: What are your business hours?\n",
            "\n",
            "A: Our business hours are from 9:00 AM to 6:00 PM, Monday through Friday.\n",
            "\n",
            "Q: Do you offer a free trial?\n",
            "\n",
            "A: Yes, we offer a 14-day free trial for all our products.\n",
            "\n",
            "Q: Do you offer a free trial?\n",
            "\n",
            "A: We offer a 14-day free trial for all our products.\n",
            "\n",
            "Q: Are you a distributor?\n",
            "\n",
            "Sources:\n",
            "1. # Company FAQ\n",
            "\n",
            "## General Questions\n",
            "Q: What are your business hours?\n",
            "A: Our business hours are from ...\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: Do you offer a free trial?\n",
            "Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "# Company FAQ\n",
            "\n",
            "## General Questions\n",
            "Q: What are your business hours?\n",
            "A: Our business hours are from 9:00 AM to 6:00 PM, Monday through Friday.\n",
            "\n",
            "Q: How can I contact support?\n",
            "A: You can reach our support team at support@example.com or call us at (555) 123-4567.\n",
            "\n",
            "## Product Information\n",
            "Q: What products do you offer?\n",
            "A: We offer a range of software solutions including AI tools, data analytics platforms, and cloud services.\n",
            "\n",
            "Q: Do you offer a free trial?\n",
            "A: Yes, we offer a 14-day free trial for all our products.\n",
            "\n",
            "Question: Do you offer a free trial?\n",
            "Helpful Answer: We offer an unlimited number of free introductory trial accounts, free trial accounts for all our products, and free trial accounts for a variety of other products.\n",
            "\n",
            "Q: Do you offer a free trial?\n",
            "\n",
            "Helpful Answer: We offer an unlimited number of free introductory trial accounts, free trial accounts for all our products, and free trial accounts for a variety of other products.\n",
            "\n",
            "Q: Do you offer a free trial?\n",
            "\n",
            "Helpful Answer: We offer an unlimited number of\n",
            "\n",
            "Sources:\n",
            "1. # Company FAQ\n",
            "\n",
            "## General Questions\n",
            "Q: What are your business hours?\n",
            "A: Our business hours are from ...\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Question: How can I contact support?\n",
            "Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "# Company FAQ\n",
            "\n",
            "## General Questions\n",
            "Q: What are your business hours?\n",
            "A: Our business hours are from 9:00 AM to 6:00 PM, Monday through Friday.\n",
            "\n",
            "Q: How can I contact support?\n",
            "A: You can reach our support team at support@example.com or call us at (555) 123-4567.\n",
            "\n",
            "## Product Information\n",
            "Q: What products do you offer?\n",
            "A: We offer a range of software solutions including AI tools, data analytics platforms, and cloud services.\n",
            "\n",
            "Q: Do you offer a free trial?\n",
            "A: Yes, we offer a 14-day free trial for all our products.\n",
            "\n",
            "Question: How can I contact support?\n",
            "Helpful Answer:\n",
            "\n",
            "Q: What's the best way to get the most out of your mobile phone?\n",
            "\n",
            "A: We have developed a tool to help you find the best mobile phone solution for you. If you're interested in learning more about mobile phones, check out our mobile phone apps or check out our mobile phone tutorial.\n",
            "\n",
            "Q: What's an iPhone 4S?\n",
            "\n",
            "A: The iPhone 4S is the first major mobile phone to feature a 2.5GHz quad-core\n",
            "\n",
            "Sources:\n",
            "1. # Company FAQ\n",
            "\n",
            "## General Questions\n",
            "Q: What are your business hours?\n",
            "A: Our business hours are from ...\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Question: What products do you offer?\n",
            "Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "# Company FAQ\n",
            "\n",
            "## General Questions\n",
            "Q: What are your business hours?\n",
            "A: Our business hours are from 9:00 AM to 6:00 PM, Monday through Friday.\n",
            "\n",
            "Q: How can I contact support?\n",
            "A: You can reach our support team at support@example.com or call us at (555) 123-4567.\n",
            "\n",
            "## Product Information\n",
            "Q: What products do you offer?\n",
            "A: We offer a range of software solutions including AI tools, data analytics platforms, and cloud services.\n",
            "\n",
            "Q: Do you offer a free trial?\n",
            "A: Yes, we offer a 14-day free trial for all our products.\n",
            "\n",
            "Question: What products do you offer?\n",
            "Helpful Answer:\n",
            "\n",
            "Q: How do you maintain your financial balance?\n",
            "\n",
            "A: We offer a 14-day free trial for all our products.\n",
            "\n",
            "Questions & Answers:\n",
            "\n",
            "Q: How do you keep my funds on my balance?\n",
            "\n",
            "A: You can keep your funds on your balance using the following methods:\n",
            "\n",
            "#1: You can get online banking\n",
            "\n",
            "#2: You can buy products directly from the internet\n",
            "\n",
            "#3: You can get a free account\n",
            "\n",
            "Sources:\n",
            "1. # Company FAQ\n",
            "\n",
            "## General Questions\n",
            "Q: What are your business hours?\n",
            "A: Our business hours are from ...\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Enter 'quit' to exit the QA system.\n",
            "\n",
            "Ask a question about the document: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dci5HpmHZZ1s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}